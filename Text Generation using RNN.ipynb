{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c3cbe7",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41095efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c276f2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1122304/1115394 [==============================] - 1s 1us/step\n",
      "1130496/1115394 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ffc4965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f24975b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7eab94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb014c",
   "metadata": {},
   "source": [
    "# Process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35483fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73412467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  ' ' :   1,\n",
      "  '!' :   2,\n",
      "  '$' :   3,\n",
      "  '&' :   4,\n",
      "  \"'\" :   5,\n",
      "  ',' :   6,\n",
      "  '-' :   7,\n",
      "  '.' :   8,\n",
      "  '3' :   9,\n",
      "  ':' :  10,\n",
      "  ';' :  11,\n",
      "  '?' :  12,\n",
      "  'A' :  13,\n",
      "  'B' :  14,\n",
      "  'C' :  15,\n",
      "  'D' :  16,\n",
      "  'E' :  17,\n",
      "  'F' :  18,\n",
      "  'G' :  19,\n",
      "  'H' :  20,\n",
      "  'I' :  21,\n",
      "  'J' :  22,\n",
      "  'K' :  23,\n",
      "  'L' :  24,\n",
      "  'M' :  25,\n",
      "  'N' :  26,\n",
      "  'O' :  27,\n",
      "  'P' :  28,\n",
      "  'Q' :  29,\n",
      "  'R' :  30,\n",
      "  'S' :  31,\n",
      "  'T' :  32,\n",
      "  'U' :  33,\n",
      "  'V' :  34,\n",
      "  'W' :  35,\n",
      "  'X' :  36,\n",
      "  'Y' :  37,\n",
      "  'Z' :  38,\n",
      "  'a' :  39,\n",
      "  'b' :  40,\n",
      "  'c' :  41,\n",
      "  'd' :  42,\n",
      "  'e' :  43,\n",
      "  'f' :  44,\n",
      "  'g' :  45,\n",
      "  'h' :  46,\n",
      "  'i' :  47,\n",
      "  'j' :  48,\n",
      "  'k' :  49,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(50)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "508de965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen' ---- characters mapped to int ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4acd06",
   "metadata": {},
   "source": [
    "# The prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fb67080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence you want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(10):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edbd16ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "47\n",
      "56\n",
      "57\n",
      "58\n",
      "1\n",
      "15\n",
      "47\n",
      "58\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "for i in char_dataset.take(10):\n",
    "    print(i.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34632e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0349142f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56c9dad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target data: 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32ed1756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 18 ('F')\n",
      "  expected output: 47 ('i')\n",
      "Step    1\n",
      "  input: 47 ('i')\n",
      "  expected output: 56 ('r')\n",
      "Step    2\n",
      "  input: 56 ('r')\n",
      "  expected output: 57 ('s')\n",
      "Step    3\n",
      "  input: 57 ('s')\n",
      "  expected output: 58 ('t')\n",
      "Step    4\n",
      "  input: 58 ('t')\n",
      "  expected output: 1 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf89d55",
   "metadata": {},
   "source": [
    "## Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74398719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int32, name=None), TensorSpec(shape=(64, 100), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a2b8c9",
   "metadata": {},
   "source": [
    "# Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b54c3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b541328",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "950548f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0c2902",
   "metadata": {},
   "source": [
    "# Try the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37c2c4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-2.48086732e-03  1.00823827e-02  1.13818590e-02 ...  3.05997115e-03\n",
      "    4.07742010e-03 -1.05883852e-02]\n",
      "  [ 5.99613297e-04  2.15811990e-02 -3.25755170e-03 ... -1.03496239e-02\n",
      "   -5.05751092e-03 -9.13000759e-03]\n",
      "  [ 1.18761638e-03  1.18700573e-02  3.50734126e-03 ... -1.02916593e-03\n",
      "   -1.02098752e-03 -1.62298828e-02]\n",
      "  ...\n",
      "  [ 1.04224812e-02  3.39803472e-03  2.98181688e-03 ... -1.16551267e-02\n",
      "   -1.50435427e-02  1.09347291e-02]\n",
      "  [ 7.82824866e-03 -3.96170001e-03  7.95410387e-03 ...  8.84433370e-03\n",
      "   -1.27658173e-02 -1.85273297e-03]\n",
      "  [-6.33857027e-03 -1.00535760e-02  5.80833899e-03 ...  1.06511265e-02\n",
      "   -3.73602449e-03 -4.40566242e-03]]\n",
      "\n",
      " [[-2.17783032e-03  8.54960177e-04 -1.41076753e-02 ... -2.62381951e-03\n",
      "    6.90550637e-03 -9.70943458e-03]\n",
      "  [ 5.17665781e-03 -3.56756384e-04 -8.55453778e-03 ...  1.45255364e-02\n",
      "    5.42390347e-03 -5.69553161e-03]\n",
      "  [ 2.13938970e-02 -3.31166433e-04 -4.68946900e-03 ...  7.72979762e-03\n",
      "    3.63833550e-03 -8.55874550e-03]\n",
      "  ...\n",
      "  [ 9.73859150e-03 -3.48022510e-03  1.30482540e-02 ...  5.69470786e-03\n",
      "    1.02542024e-02 -1.25989560e-02]\n",
      "  [ 1.20690493e-02 -5.05128829e-03  1.28766667e-04 ...  2.02083085e-02\n",
      "    9.17890761e-03  4.42623161e-04]\n",
      "  [ 3.16152023e-03 -6.38351636e-03  4.94787563e-03 ...  2.89147217e-02\n",
      "   -5.70974499e-03 -7.10081309e-04]]\n",
      "\n",
      " [[ 8.44011083e-03 -8.12678598e-04  3.24903848e-03 ... -1.62071250e-02\n",
      "    5.10347867e-03  1.30467711e-03]\n",
      "  [ 9.94967204e-03  1.20467506e-02  1.20231686e-02 ...  4.28940030e-03\n",
      "    7.39074824e-03  3.67459096e-03]\n",
      "  [ 1.38242505e-02  7.90985394e-03  9.92826372e-03 ... -5.14645549e-03\n",
      "   -1.15482295e-02  1.22761996e-02]\n",
      "  ...\n",
      "  [ 7.86113273e-03  8.83152476e-04  3.57095851e-05 ... -2.77618319e-03\n",
      "   -1.29336109e-02  1.49046946e-02]\n",
      "  [ 8.46231170e-03 -2.81491037e-03  6.75009750e-03 ... -6.27001235e-03\n",
      "   -3.15557537e-03  7.38722086e-03]\n",
      "  [ 1.58583105e-03 -2.76735984e-04 -1.84775610e-03 ...  1.85781568e-02\n",
      "    8.93425103e-03 -7.87261687e-03]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.32643329e-02  1.06062293e-02 -3.83180054e-03 ...  1.36900423e-02\n",
      "   -2.98636081e-03  3.93190142e-03]\n",
      "  [ 2.49554077e-03  3.39404494e-02 -3.30222864e-03 ...  1.09023871e-02\n",
      "   -8.39168765e-03  5.52002247e-03]\n",
      "  [-6.35507097e-03  1.14841275e-02 -2.18854682e-03 ...  7.06908014e-03\n",
      "   -5.17824665e-04 -1.43183500e-03]\n",
      "  ...\n",
      "  [ 9.92408395e-03 -2.30947975e-04  1.71385575e-04 ...  9.71157942e-03\n",
      "   -1.36624544e-03 -4.34660632e-03]\n",
      "  [ 2.41753110e-03 -4.76700906e-03  5.13381790e-03 ...  2.92126704e-02\n",
      "   -8.63004755e-03  3.71566578e-03]\n",
      "  [-6.19140547e-03 -1.06151653e-02  3.88786197e-03 ...  2.23863628e-02\n",
      "   -3.53395473e-04 -1.27814244e-04]]\n",
      "\n",
      " [[-2.65646749e-03  6.96118921e-03  2.09734146e-03 ...  9.59526841e-03\n",
      "    8.80870130e-03 -1.37963984e-03]\n",
      "  [-4.58121393e-03  1.21910395e-02 -6.06063288e-03 ...  4.17286018e-03\n",
      "    4.87913936e-03 -3.69279343e-03]\n",
      "  [ 9.87687707e-03  5.97753655e-03  9.32393130e-04 ... -4.55508940e-03\n",
      "   -1.29087223e-02  1.21577317e-02]\n",
      "  ...\n",
      "  [ 7.77250063e-03  9.26401000e-03 -5.10769989e-03 ...  9.63527802e-03\n",
      "    4.01991233e-03  8.14281963e-03]\n",
      "  [-2.02378863e-03 -3.98157118e-03  1.22847408e-02 ...  6.54548407e-03\n",
      "   -1.50824012e-03 -1.29126692e-02]\n",
      "  [ 1.09017147e-02  8.71240627e-04  1.11666610e-02 ... -4.97471541e-03\n",
      "   -1.48056671e-02  5.40851522e-03]]\n",
      "\n",
      " [[ 5.65581163e-03  1.22271832e-02  9.57957003e-03 ...  1.37588289e-02\n",
      "    5.60992397e-03  4.29053046e-03]\n",
      "  [-5.74197387e-03 -1.49573013e-03  1.81946233e-02 ...  1.01350332e-02\n",
      "   -1.20552746e-03 -1.29308542e-02]\n",
      "  [ 7.61217438e-03  2.65867962e-03  1.32074719e-02 ... -2.47989851e-03\n",
      "   -1.54177630e-02  5.79522736e-03]\n",
      "  ...\n",
      "  [ 1.54985813e-02  1.14970449e-02  3.69539484e-03 ... -3.84217594e-03\n",
      "   -1.16754370e-02 -4.46097599e-03]\n",
      "  [ 1.79199539e-02  7.07741128e-03  5.06478176e-03 ... -6.31461944e-03\n",
      "   -2.13828143e-02  1.21335546e-02]\n",
      "  [ 9.82629694e-03  2.26067118e-02 -5.25505329e-03 ... -1.59384459e-02\n",
      "   -2.00650580e-02  3.08523164e-03]]], shape=(64, 100, 65), dtype=float32) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0369127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           16640     \n",
      "                                                                 \n",
      " gru (GRU)                   (64, None, 1024)          3938304   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 65)            66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e9bed65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8128dac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41, 16, 61, 27, 58,  1, 13, 38, 33, 45, 37, 41, 17, 21, 16, 23, 62,\n",
       "       10, 33, 35, 17, 57, 56, 32, 31, 12, 51, 45, 46, 63, 28, 51, 47, 29,\n",
       "       19, 20, 35, 27, 41, 44, 15,  9, 55, 50, 27, 13, 48, 51,  6, 34, 52,\n",
       "       39, 43, 18, 50,  8, 33, 59, 44, 56, 19, 23, 19, 42, 59, 11,  3, 32,\n",
       "        1, 34, 43,  9, 53, 63, 35, 63,  5, 11, 56,  4, 47,  9, 54, 20, 42,\n",
       "       26, 23, 22,  9, 30, 60, 32, 46, 39, 20, 38, 30, 29, 31,  3],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba89af7",
   "metadata": {},
   "source": [
    "Decode these to see the text predicted by this untrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d9ae634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " \"el's monument.\\n\\nBALTHASAR:\\nIt doth so, holy sir; and there's my master,\\nOne that you love.\\n\\nFRIAR LA\"\n",
      "\n",
      "Next Char Predictions: \n",
      " \"cDwOt AZUgYcEIDKx:UWEsrTS?mghyPmiQGHWOcfC3qlOAjm,VnaeFl.UufrGKGdu;$T Ve3oyWy';r&i3pHdNKJ3RvThaHZRQS$\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ff9f8",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c1deeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 65)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.174078\n"
     ]
    }
   ],
   "source": [
    "# Attach an optimizer, and a loss function\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e29686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6f1df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5276f1",
   "metadata": {},
   "source": [
    "## Execute the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41cd8775",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d599447d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "172/172 [==============================] - 1081s 6s/step - loss: 2.4212\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 847s 5s/step - loss: 1.9083\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 799s 5s/step - loss: 1.6612\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 749s 4s/step - loss: 1.5245\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 695s 4s/step - loss: 1.4429\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 686s 4s/step - loss: 1.3873\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 793s 5s/step - loss: 1.3415\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - 662s 4s/step - loss: 1.3033\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - 796s 5s/step - loss: 1.2704\n",
      "Epoch 10/10\n",
      "172/172 [==============================] - 796s 5s/step - loss: 1.2380\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be76c6c",
   "metadata": {},
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1532a901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt_10'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5079917",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87eb4aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (1, None, 256)            16640     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (1, None, 1024)           3938304   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, None, 65)             66625     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "531f8ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 2000\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperature results in more predictable text.\n",
    "    # Higher temperature results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "246296c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUYYOUS:\n",
      "Varrier, nollibe,\n",
      "Confiration shoes with such a tobbling\n",
      "By: I be\n",
      "Satisf,' sands, 'War ience will be to her;\n",
      "To tell what I prove all, daughter.\n",
      "\n",
      "LUCIO:\n",
      "'Tis a visite live, ir hatcheghat a pretty pirce;\n",
      "But now the peace of suafty ragler, I\n",
      "descing it contrive, to father is emploces:\n",
      "Tabeak up two leds that leave your foil:\n",
      "If he quit Edward will both make two brother;\n",
      "So stoow the nobleside to tear from him,\n",
      "To make this lack of answer,\n",
      "his lips blood is like no rem mine.\n",
      "\n",
      "MENENIUS:\n",
      "peace;\n",
      "To hell for this young guilty of\n",
      "thy father should I wonder,\n",
      "But in the worfard to venge of twail.\n",
      "\n",
      "LUCIO:\n",
      "Our roy is a notething, lie know me\n",
      "To stille my honour's love, and safe.\n",
      "\n",
      "CORIOLANUS:\n",
      "Master came to Prince's blush!\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "KING RICHARD III:\n",
      "Bragge your beholding to thy certions\n",
      "Have bees deaves done so love a flower.\n",
      "\n",
      "GLOUCESTER:\n",
      "Camillo, My fortune constast what kissmy too?\n",
      "\n",
      "JULIET:\n",
      "The tribunes. Now he could scarce to see me\n",
      "Even bows, Claudis't be teeming, I have beent\n",
      "for cause, and ven you bless us!\n",
      "\n",
      "Second Citizen:\n",
      "Here comes the course\n",
      "And serve him an aged affection.\n",
      "\n",
      "VINCENTIO:\n",
      "Is God will must dance.\n",
      "\n",
      "PAULINA:\n",
      "I shall, in a formitful handman.\n",
      "\n",
      "BIANCA:\n",
      "How! Think, if every crowns.\n",
      "\n",
      "KING RICHARD III:\n",
      "Say, I pass'd; follow me with him.\n",
      "\n",
      "QUEEN MARDARET:\n",
      "Tuftly be rough lift unto him wanting.\n",
      "\n",
      "GONZALO will he fram blood\n",
      "Should so.\n",
      "\n",
      "ROMEO:\n",
      "Out of thee, since; for your voices!\n",
      "\n",
      "HORTENSIO:\n",
      "Soft! I\n",
      "have been, my father comes that is a husband's death,\n",
      "I'll make can live thy life resign upon\n",
      "'Tis sake, lets me to me?\n",
      "\n",
      "WARWICK:\n",
      "Fear I see it?\n",
      "He write on suitor, who shalw you steal, means?\n",
      "\n",
      "SEBASTIAN:\n",
      "Hold you hence.\n",
      "\n",
      "First Honourable light\n",
      "And send me wolp to slander out,\n",
      "For in himsol and whits shames?\n",
      "What says, would attend upon off witcects that Holvast given to set other find\n",
      "your bed; the goose friend.\n",
      "\n",
      "KING RICHARD III:\n",
      "Wrong about resire.\n",
      "\n",
      "QUEEN ELBO. my done! Ellow, sir, is no not a lain\n",
      "And prick again and but flecked joy.\n",
      "\n",
      "BIONDE:\n",
      "Your lords and\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"Q\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf709aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
